{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learn to play at Breakout \n",
    "\n",
    "### Requirements\n",
    "\n",
    "- In the repo root directory, do `pipenv install --dev` \n",
    "- Or, install the needed packages:\n",
    "\n",
    "      pip install keras-rl gym_breakout_pygame keras tensorflow-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%import numpy as np\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "pygame 1.9.6\nHello from the pygame community. https://www.pygame.org/contribute.html\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from gym.wrappers import Monitor\n",
    "from gym_breakout_pygame.wrappers.normal_space import BreakoutNMultiDiscrete\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(5,)\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_2 (Flatten)          (None, 5)                 0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                384       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 64)                4160      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 3)                 195       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 4,739\nTrainable params: 4,739\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "env = BreakoutNMultiDiscrete()\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "print(env.observation_space.shape)\n",
    "window_length = 4\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(window_length,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training for 50000 steps ...\nWARNING:tensorflow:From /home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\n",
      "   106/50000: episode: 1, duration: 1.571s, episode steps: 106, steps per second: 67, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.208 [0.000, 2.000], mean observation: 8.772 [0.000, 47.000], loss: 0.310973, mae: 0.631189, mean_q: -0.047010\n",
      "   313/50000: episode: 2, duration: 1.383s, episode steps: 207, steps per second: 150, episode reward: 10.000, mean reward: 0.048 [0.000, 5.000], mean action: 1.019 [0.000, 2.000], mean observation: 8.131 [0.000, 47.000], loss: 0.167612, mae: 0.392361, mean_q: 0.022715\n   328/50000: episode: 3, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.067 [0.000, 2.000], mean observation: 11.307 [1.000, 47.000], loss: 0.101809, mae: 0.352385, mean_q: 0.123184\n",
      "   343/50000: episode: 4, duration: 0.106s, episode steps: 15, steps per second: 142, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.000 [0.000, 2.000], mean observation: 11.707 [1.000, 47.000], loss: 0.108616, mae: 0.399974, mean_q: 0.137595\n",
      "   450/50000: episode: 5, duration: 0.752s, episode steps: 107, steps per second: 142, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.009 [0.000, 2.000], mean observation: 9.359 [0.000, 47.000], loss: 0.090905, mae: 0.386982, mean_q: 0.104728\n",
      "   560/50000: episode: 6, duration: 0.800s, episode steps: 110, steps per second: 138, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 1.100 [0.000, 2.000], mean observation: 9.360 [0.000, 47.000], loss: 0.106871, mae: 0.411816, mean_q: 0.224008\n",
      "   755/50000: episode: 7, duration: 1.341s, episode steps: 195, steps per second: 145, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.974 [0.000, 2.000], mean observation: 8.718 [0.000, 47.000], loss: 0.086383, mae: 0.442157, mean_q: 0.323414\n   771/50000: episode: 8, duration: 0.111s, episode steps: 16, steps per second: 144, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.125 [0.000, 2.000], mean observation: 11.537 [1.000, 47.000], loss: 0.067495, mae: 0.428166, mean_q: 0.375988\n",
      "   873/50000: episode: 9, duration: 0.715s, episode steps: 102, steps per second: 143, episode reward: 5.000, mean reward: 0.049 [0.000, 5.000], mean action: 0.892 [0.000, 2.000], mean observation: 9.696 [0.000, 47.000], loss: 0.062934, mae: 0.484691, mean_q: 0.478129\n   886/50000: episode: 10, duration: 0.092s, episode steps: 13, steps per second: 142, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.923 [0.000, 2.000], mean observation: 11.400 [0.000, 47.000], loss: 0.122295, mae: 0.552583, mean_q: 0.617238\n",
      "  1072/50000: episode: 11, duration: 1.255s, episode steps: 186, steps per second: 148, episode reward: 10.000, mean reward: 0.054 [0.000, 5.000], mean action: 0.989 [0.000, 2.000], mean observation: 8.884 [0.000, 47.000], loss: 0.077192, mae: 0.583497, mean_q: 0.703277\n  1087/50000: episode: 12, duration: 0.101s, episode steps: 15, steps per second: 148, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.067 [0.000, 2.000], mean observation: 11.480 [1.000, 47.000], loss: 0.082098, mae: 0.589657, mean_q: 0.725343\n",
      "  1279/50000: episode: 13, duration: 1.320s, episode steps: 192, steps per second: 145, episode reward: 10.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.010 [0.000, 2.000], mean observation: 9.474 [0.000, 47.000], loss: 0.053961, mae: 0.727300, mean_q: 0.985558\n",
      "  1384/50000: episode: 14, duration: 0.708s, episode steps: 105, steps per second: 148, episode reward: 5.000, mean reward: 0.048 [0.000, 5.000], mean action: 1.067 [0.000, 2.000], mean observation: 10.110 [0.000, 47.000], loss: 0.052364, mae: 0.855576, mean_q: 1.240715\n",
      "  1494/50000: episode: 15, duration: 0.785s, episode steps: 110, steps per second: 140, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.927 [0.000, 2.000], mean observation: 8.682 [0.000, 47.000], loss: 0.046454, mae: 0.957322, mean_q: 1.414197\n",
      "  1712/50000: episode: 16, duration: 1.549s, episode steps: 218, steps per second: 141, episode reward: 10.000, mean reward: 0.046 [0.000, 5.000], mean action: 1.046 [0.000, 2.000], mean observation: 8.948 [0.000, 47.000], loss: 0.059379, mae: 1.000027, mean_q: 1.503178\n",
      "  2009/50000: episode: 17, duration: 2.016s, episode steps: 297, steps per second: 147, episode reward: 15.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.064 [0.000, 2.000], mean observation: 9.397 [0.000, 47.000], loss: 0.057384, mae: 1.291365, mean_q: 1.990674\n",
      "  2221/50000: episode: 18, duration: 1.503s, episode steps: 212, steps per second: 141, episode reward: 10.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.104 [0.000, 2.000], mean observation: 9.357 [0.000, 47.000], loss: 0.066056, mae: 1.556687, mean_q: 2.432395\n",
      "  2415/50000: episode: 19, duration: 1.314s, episode steps: 194, steps per second: 148, episode reward: 10.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.067 [0.000, 2.000], mean observation: 9.584 [0.000, 47.000], loss: 0.076186, mae: 1.799874, mean_q: 2.813347\n",
      "  2519/50000: episode: 20, duration: 0.705s, episode steps: 104, steps per second: 147, episode reward: 5.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.942 [0.000, 2.000], mean observation: 8.637 [0.000, 47.000], loss: 0.102151, mae: 1.940177, mean_q: 3.052276\n  2535/50000: episode: 21, duration: 0.109s, episode steps: 16, steps per second: 146, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.750 [0.000, 2.000], mean observation: 11.512 [1.000, 47.000], loss: 0.179151, mae: 1.964864, mean_q: 3.170171\n",
      "  3025/50000: episode: 22, duration: 3.260s, episode steps: 490, steps per second: 150, episode reward: 25.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.088 [0.000, 2.000], mean observation: 9.194 [0.000, 47.000], loss: 0.111412, mae: 2.329120, mean_q: 3.626705\n",
      "  3325/50000: episode: 23, duration: 1.980s, episode steps: 300, steps per second: 152, episode reward: 15.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.067 [0.000, 2.000], mean observation: 8.691 [0.000, 47.000], loss: 0.135088, mae: 2.768465, mean_q: 4.285166\n",
      "  3905/50000: episode: 24, duration: 3.868s, episode steps: 580, steps per second: 150, episode reward: 45.000, mean reward: 0.078 [0.000, 5.000], mean action: 0.998 [0.000, 2.000], mean observation: 7.641 [0.000, 46.000], loss: 0.208492, mae: 3.177193, mean_q: 4.866774\n",
      "  4008/50000: episode: 25, duration: 0.726s, episode steps: 103, steps per second: 142, episode reward: 5.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.000 [0.000, 2.000], mean observation: 9.777 [0.000, 47.000], loss: 0.236294, mae: 3.469547, mean_q: 5.293877\n",
      "  4119/50000: episode: 26, duration: 0.804s, episode steps: 111, steps per second: 138, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 1.045 [0.000, 2.000], mean observation: 9.216 [0.000, 47.000], loss: 0.335977, mae: 3.501416, mean_q: 5.335428\n",
      "  4427/50000: episode: 27, duration: 2.066s, episode steps: 308, steps per second: 149, episode reward: 15.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.091 [0.000, 2.000], mean observation: 8.641 [0.000, 47.000], loss: 0.284924, mae: 3.686650, mean_q: 5.607796\n",
      "  4623/50000: episode: 28, duration: 1.350s, episode steps: 196, steps per second: 145, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.087 [0.000, 2.000], mean observation: 9.324 [0.000, 47.000], loss: 0.232468, mae: 3.840339, mean_q: 5.845484\n",
      "  4922/50000: episode: 29, duration: 2.005s, episode steps: 299, steps per second: 149, episode reward: 15.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.030 [0.000, 2.000], mean observation: 8.656 [0.000, 47.000], loss: 0.236627, mae: 3.961601, mean_q: 6.014982\n",
      "  5220/50000: episode: 30, duration: 2.053s, episode steps: 298, steps per second: 145, episode reward: 15.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.030 [0.000, 2.000], mean observation: 8.981 [0.000, 47.000], loss: 0.271233, mae: 4.086070, mean_q: 6.184616\n",
      "  5320/50000: episode: 31, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 5.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.020 [0.000, 2.000], mean observation: 9.608 [0.000, 47.000], loss: 0.259679, mae: 4.245007, mean_q: 6.401895\n",
      "  5744/50000: episode: 32, duration: 3.001s, episode steps: 424, steps per second: 141, episode reward: 40.000, mean reward: 0.094 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 8.722 [0.000, 47.000], loss: 0.267795, mae: 4.323489, mean_q: 6.544261\n",
      "  5932/50000: episode: 33, duration: 1.282s, episode steps: 188, steps per second: 147, episode reward: 10.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.963 [0.000, 2.000], mean observation: 8.661 [0.000, 47.000], loss: 0.302545, mae: 4.501383, mean_q: 6.816855\n",
      "  6042/50000: episode: 34, duration: 0.759s, episode steps: 110, steps per second: 145, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.927 [0.000, 2.000], mean observation: 9.055 [0.000, 47.000], loss: 0.236342, mae: 4.508925, mean_q: 6.818267\n  6059/50000: episode: 35, duration: 0.119s, episode steps: 17, steps per second: 143, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.353 [0.000, 2.000], mean observation: 12.106 [0.000, 47.000], loss: 0.248312, mae: 4.400824, mean_q: 6.691044\n",
      "  6247/50000: episode: 36, duration: 1.329s, episode steps: 188, steps per second: 142, episode reward: 10.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.941 [0.000, 2.000], mean observation: 8.743 [0.000, 47.000], loss: 0.341446, mae: 4.587339, mean_q: 6.955792\n",
      "  6439/50000: episode: 37, duration: 1.295s, episode steps: 192, steps per second: 148, episode reward: 10.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.016 [0.000, 2.000], mean observation: 8.022 [0.000, 47.000], loss: 0.310050, mae: 4.691114, mean_q: 7.093416\n  6455/50000: episode: 38, duration: 0.109s, episode steps: 16, steps per second: 147, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.875 [0.000, 2.000], mean observation: 11.238 [0.000, 47.000], loss: 0.285551, mae: 4.643152, mean_q: 6.980094\n",
      "  6658/50000: episode: 39, duration: 1.395s, episode steps: 203, steps per second: 146, episode reward: 10.000, mean reward: 0.049 [0.000, 5.000], mean action: 0.956 [0.000, 2.000], mean observation: 8.130 [0.000, 47.000], loss: 0.270421, mae: 4.754786, mean_q: 7.180728\n  6672/50000: episode: 40, duration: 0.098s, episode steps: 14, steps per second: 143, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.357 [0.000, 2.000], mean observation: 12.100 [0.000, 47.000], loss: 0.269550, mae: 4.865014, mean_q: 7.332148\n",
      "  7325/50000: episode: 41, duration: 4.321s, episode steps: 653, steps per second: 151, episode reward: 45.000, mean reward: 0.069 [0.000, 5.000], mean action: 1.032 [0.000, 2.000], mean observation: 8.255 [0.000, 47.000], loss: 0.299536, mae: 4.963197, mean_q: 7.487158\n",
      "  7523/50000: episode: 42, duration: 1.334s, episode steps: 198, steps per second: 148, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.919 [0.000, 2.000], mean observation: 8.423 [0.000, 47.000], loss: 0.326766, mae: 5.196308, mean_q: 7.825928\n",
      "  7827/50000: episode: 43, duration: 2.000s, episode steps: 304, steps per second: 152, episode reward: 15.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.030 [0.000, 2.000], mean observation: 9.066 [0.000, 47.000], loss: 0.372281, mae: 5.277569, mean_q: 7.951109\n",
      "  8022/50000: episode: 44, duration: 1.324s, episode steps: 195, steps per second: 147, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.021 [0.000, 2.000], mean observation: 8.885 [0.000, 47.000], loss: 0.346775, mae: 5.365116, mean_q: 8.087339\n",
      "  8205/50000: episode: 45, duration: 1.244s, episode steps: 183, steps per second: 147, episode reward: 10.000, mean reward: 0.055 [0.000, 5.000], mean action: 0.984 [0.000, 2.000], mean observation: 9.163 [0.000, 47.000], loss: 0.322332, mae: 5.432150, mean_q: 8.179289\n",
      "  8325/50000: episode: 46, duration: 0.926s, episode steps: 120, steps per second: 130, episode reward: 5.000, mean reward: 0.042 [0.000, 5.000], mean action: 1.067 [0.000, 2.000], mean observation: 9.522 [0.000, 47.000], loss: 0.286376, mae: 5.469665, mean_q: 8.241403\n",
      "  8604/50000: episode: 47, duration: 1.874s, episode steps: 279, steps per second: 149, episode reward: 15.000, mean reward: 0.054 [0.000, 5.000], mean action: 1.018 [0.000, 2.000], mean observation: 8.270 [0.000, 47.000], loss: 0.321103, mae: 5.540228, mean_q: 8.336378\n",
      "  8823/50000: episode: 48, duration: 1.483s, episode steps: 219, steps per second: 148, episode reward: 10.000, mean reward: 0.046 [0.000, 5.000], mean action: 0.977 [0.000, 2.000], mean observation: 8.516 [0.000, 47.000], loss: 0.293948, mae: 5.653645, mean_q: 8.520161\n",
      "  8926/50000: episode: 49, duration: 0.719s, episode steps: 103, steps per second: 143, episode reward: 5.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.019 [0.000, 2.000], mean observation: 9.581 [0.000, 47.000], loss: 0.289684, mae: 5.756982, mean_q: 8.672555\n",
      "  9033/50000: episode: 50, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.150 [0.000, 2.000], mean observation: 9.748 [0.000, 47.000], loss: 0.381158, mae: 5.752188, mean_q: 8.662105\n",
      "  9136/50000: episode: 51, duration: 0.723s, episode steps: 103, steps per second: 143, episode reward: 5.000, mean reward: 0.049 [0.000, 5.000], mean action: 0.942 [0.000, 2.000], mean observation: 9.668 [0.000, 47.000], loss: 0.360383, mae: 5.738585, mean_q: 8.640656\n",
      "  9336/50000: episode: 52, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: 10.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.115 [0.000, 2.000], mean observation: 8.775 [0.000, 47.000], loss: 0.405490, mae: 5.713137, mean_q: 8.568480\n",
      "  9976/50000: episode: 53, duration: 4.242s, episode steps: 640, steps per second: 151, episode reward: 45.000, mean reward: 0.070 [0.000, 5.000], mean action: 1.036 [0.000, 2.000], mean observation: 8.392 [0.000, 47.000], loss: 0.356749, mae: 5.783622, mean_q: 8.707899\n",
      " 10166/50000: episode: 54, duration: 1.275s, episode steps: 190, steps per second: 149, episode reward: 10.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.958 [0.000, 2.000], mean observation: 8.559 [0.000, 47.000], loss: 0.428254, mae: 5.961320, mean_q: 8.974580\n",
      " 10656/50000: episode: 55, duration: 3.276s, episode steps: 490, steps per second: 150, episode reward: 25.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.053 [0.000, 2.000], mean observation: 8.738 [0.000, 47.000], loss: 0.413664, mae: 6.036134, mean_q: 9.097646\n 10670/50000: episode: 56, duration: 0.099s, episode steps: 14, steps per second: 141, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.929 [0.000, 2.000], mean observation: 11.186 [1.000, 47.000], loss: 0.274702, mae: 6.190434, mean_q: 9.361862\n",
      " 11055/50000: episode: 57, duration: 2.810s, episode steps: 385, steps per second: 137, episode reward: 20.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.091 [0.000, 2.000], mean observation: 9.096 [0.000, 47.000], loss: 0.386342, mae: 6.284556, mean_q: 9.505306\n",
      " 11255/50000: episode: 58, duration: 1.522s, episode steps: 200, steps per second: 131, episode reward: 10.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.035 [0.000, 2.000], mean observation: 8.824 [0.000, 47.000], loss: 0.409813, mae: 6.496442, mean_q: 9.806345\n",
      " 11625/50000: episode: 59, duration: 2.864s, episode steps: 370, steps per second: 129, episode reward: 20.000, mean reward: 0.054 [0.000, 5.000], mean action: 1.005 [0.000, 2.000], mean observation: 8.451 [0.000, 47.000], loss: 0.421652, mae: 6.650272, mean_q: 10.053712\n",
      " 12119/50000: episode: 60, duration: 3.943s, episode steps: 494, steps per second: 125, episode reward: 35.000, mean reward: 0.071 [0.000, 5.000], mean action: 0.996 [0.000, 2.000], mean observation: 8.794 [0.000, 47.000], loss: 0.417279, mae: 6.900662, mean_q: 10.433929\n",
      " 12677/50000: episode: 61, duration: 4.375s, episode steps: 558, steps per second: 128, episode reward: 45.000, mean reward: 0.081 [0.000, 5.000], mean action: 1.104 [0.000, 2.000], mean observation: 8.085 [0.000, 46.000], loss: 0.461783, mae: 7.257109, mean_q: 10.961442\n 12694/50000: episode: 62, duration: 0.196s, episode steps: 17, steps per second: 87, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.235 [0.000, 2.000], mean observation: 12.165 [0.000, 47.000], loss: 0.555807, mae: 7.268845, mean_q: 10.974069\n",
      " 13333/50000: episode: 63, duration: 4.994s, episode steps: 639, steps per second: 128, episode reward: 45.000, mean reward: 0.070 [0.000, 5.000], mean action: 1.052 [0.000, 2.000], mean observation: 8.298 [0.000, 46.000], loss: 0.539502, mae: 7.637661, mean_q: 11.531110\n 13348/50000: episode: 64, duration: 0.133s, episode steps: 15, steps per second: 113, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.933 [0.000, 2.000], mean observation: 11.480 [1.000, 47.000], loss: 0.419439, mae: 7.564104, mean_q: 11.437592\n",
      " 13365/50000: episode: 65, duration: 0.151s, episode steps: 17, steps per second: 112, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.059 [0.000, 2.000], mean observation: 12.188 [0.000, 47.000], loss: 0.381887, mae: 7.832687, mean_q: 11.829656\n",
      " 13560/50000: episode: 66, duration: 1.606s, episode steps: 195, steps per second: 121, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.021 [0.000, 2.000], mean observation: 8.311 [0.000, 47.000], loss: 0.548170, mae: 7.771325, mean_q: 11.731140\n",
      " 13840/50000: episode: 67, duration: 2.107s, episode steps: 280, steps per second: 133, episode reward: 15.000, mean reward: 0.054 [0.000, 5.000], mean action: 1.004 [0.000, 2.000], mean observation: 8.865 [0.000, 47.000], loss: 0.523844, mae: 7.882664, mean_q: 11.884085\n",
      " 14131/50000: episode: 68, duration: 2.072s, episode steps: 291, steps per second: 140, episode reward: 15.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.027 [0.000, 2.000], mean observation: 8.948 [0.000, 47.000], loss: 0.517986, mae: 8.037945, mean_q: 12.118306\n",
      " 14639/50000: episode: 69, duration: 3.418s, episode steps: 508, steps per second: 149, episode reward: 40.000, mean reward: 0.079 [0.000, 5.000], mean action: 1.016 [0.000, 2.000], mean observation: 8.248 [0.000, 47.000], loss: 0.524154, mae: 8.197284, mean_q: 12.363151\n",
      " 14755/50000: episode: 70, duration: 0.822s, episode steps: 116, steps per second: 141, episode reward: 5.000, mean reward: 0.043 [0.000, 5.000], mean action: 1.155 [0.000, 2.000], mean observation: 9.048 [0.000, 47.000], loss: 0.573003, mae: 8.433439, mean_q: 12.686294\n",
      " 15045/50000: episode: 71, duration: 2.242s, episode steps: 290, steps per second: 129, episode reward: 15.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.010 [0.000, 2.000], mean observation: 8.650 [0.000, 47.000], loss: 0.592005, mae: 8.435335, mean_q: 12.693547\n",
      " 15415/50000: episode: 72, duration: 2.568s, episode steps: 370, steps per second: 144, episode reward: 20.000, mean reward: 0.054 [0.000, 5.000], mean action: 1.014 [0.000, 2.000], mean observation: 8.791 [0.000, 47.000], loss: 0.468759, mae: 8.513874, mean_q: 12.815018\n",
      " 15612/50000: episode: 73, duration: 1.405s, episode steps: 197, steps per second: 140, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.888 [0.000, 2.000], mean observation: 9.179 [0.000, 47.000], loss: 0.439188, mae: 8.617848, mean_q: 12.958935\n",
      " 15722/50000: episode: 74, duration: 0.770s, episode steps: 110, steps per second: 143, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.991 [0.000, 2.000], mean observation: 9.305 [0.000, 47.000], loss: 0.545640, mae: 8.646821, mean_q: 12.970153\n",
      " 16888/50000: episode: 75, duration: 7.911s, episode steps: 1166, steps per second: 147, episode reward: 30.000, mean reward: 0.026 [0.000, 5.000], mean action: 1.077 [0.000, 2.000], mean observation: 8.008 [0.000, 47.000], loss: 0.456102, mae: 8.694634, mean_q: 13.043708\n",
      " 17083/50000: episode: 76, duration: 1.369s, episode steps: 195, steps per second: 142, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.077 [0.000, 2.000], mean observation: 8.970 [0.000, 47.000], loss: 0.458663, mae: 8.751602, mean_q: 13.140942\n",
      " 17473/50000: episode: 77, duration: 2.766s, episode steps: 390, steps per second: 141, episode reward: 20.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.982 [0.000, 2.000], mean observation: 8.597 [0.000, 47.000], loss: 0.399673, mae: 8.745261, mean_q: 13.127612\n 17487/50000: episode: 78, duration: 0.102s, episode steps: 14, steps per second: 137, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.071 [0.000, 2.000], mean observation: 12.271 [0.000, 47.000], loss: 0.738795, mae: 8.658969, mean_q: 12.865109\n",
      " 17784/50000: episode: 79, duration: 2.043s, episode steps: 297, steps per second: 145, episode reward: 15.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.051 [0.000, 2.000], mean observation: 8.797 [0.000, 47.000], loss: 0.418641, mae: 8.708218, mean_q: 13.058466\n",
      " 17897/50000: episode: 80, duration: 0.810s, episode steps: 113, steps per second: 139, episode reward: 5.000, mean reward: 0.044 [0.000, 5.000], mean action: 1.071 [0.000, 2.000], mean observation: 9.081 [0.000, 47.000], loss: 0.410202, mae: 8.730109, mean_q: 13.102495\n",
      " 18269/50000: episode: 81, duration: 2.787s, episode steps: 372, steps per second: 133, episode reward: 20.000, mean reward: 0.054 [0.000, 5.000], mean action: 0.957 [0.000, 2.000], mean observation: 8.373 [0.000, 47.000], loss: 0.333725, mae: 8.709207, mean_q: 13.073442\n",
      " 18372/50000: episode: 82, duration: 0.779s, episode steps: 103, steps per second: 132, episode reward: 5.000, mean reward: 0.049 [0.000, 5.000], mean action: 0.825 [0.000, 2.000], mean observation: 8.623 [0.000, 47.000], loss: 0.332208, mae: 8.736324, mean_q: 13.120769\n",
      " 18841/50000: episode: 83, duration: 3.316s, episode steps: 469, steps per second: 141, episode reward: 25.000, mean reward: 0.053 [0.000, 5.000], mean action: 1.019 [0.000, 2.000], mean observation: 8.912 [0.000, 47.000], loss: 0.306473, mae: 8.783262, mean_q: 13.197209\n",
      " 18944/50000: episode: 84, duration: 0.753s, episode steps: 103, steps per second: 137, episode reward: 5.000, mean reward: 0.049 [0.000, 5.000], mean action: 0.903 [0.000, 2.000], mean observation: 9.280 [0.000, 47.000], loss: 0.325030, mae: 8.808826, mean_q: 13.235044\n",
      " 19059/50000: episode: 85, duration: 0.817s, episode steps: 115, steps per second: 141, episode reward: 5.000, mean reward: 0.043 [0.000, 5.000], mean action: 0.991 [0.000, 2.000], mean observation: 9.278 [0.000, 47.000], loss: 0.390085, mae: 8.812619, mean_q: 13.200173\n",
      " 19165/50000: episode: 86, duration: 0.781s, episode steps: 106, steps per second: 136, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 0.972 [0.000, 2.000], mean observation: 9.258 [0.000, 47.000], loss: 0.372007, mae: 8.774416, mean_q: 13.149736\n",
      " 19360/50000: episode: 87, duration: 1.382s, episode steps: 195, steps per second: 141, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.005 [0.000, 2.000], mean observation: 8.593 [0.000, 47.000], loss: 0.363114, mae: 8.788863, mean_q: 13.177225\n",
      " 19464/50000: episode: 88, duration: 0.777s, episode steps: 104, steps per second: 134, episode reward: 5.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.971 [0.000, 2.000], mean observation: 9.213 [0.000, 47.000], loss: 0.320393, mae: 8.751488, mean_q: 13.145387\n",
      " 19659/50000: episode: 89, duration: 1.507s, episode steps: 195, steps per second: 129, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.015 [0.000, 2.000], mean observation: 9.103 [0.000, 47.000], loss: 0.276109, mae: 8.726158, mean_q: 13.107246\n",
      " 19771/50000: episode: 90, duration: 0.873s, episode steps: 112, steps per second: 128, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.991 [0.000, 2.000], mean observation: 9.204 [0.000, 47.000], loss: 0.378786, mae: 8.703296, mean_q: 13.061477\n 19785/50000: episode: 91, duration: 0.112s, episode steps: 14, steps per second: 124, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.286 [0.000, 2.000], mean observation: 12.171 [0.000, 47.000], loss: 0.365239, mae: 8.747484, mean_q: 13.117683\n",
      " 19895/50000: episode: 92, duration: 0.793s, episode steps: 110, steps per second: 139, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.955 [0.000, 2.000], mean observation: 9.615 [0.000, 47.000], loss: 0.273426, mae: 8.765260, mean_q: 13.158955\n",
      " 20299/50000: episode: 93, duration: 2.934s, episode steps: 404, steps per second: 138, episode reward: 20.000, mean reward: 0.050 [0.000, 5.000], mean action: 0.993 [0.000, 2.000], mean observation: 8.609 [0.000, 47.000], loss: 0.299030, mae: 8.717737, mean_q: 13.082962\n",
      " 20506/50000: episode: 94, duration: 1.456s, episode steps: 207, steps per second: 142, episode reward: 10.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.928 [0.000, 2.000], mean observation: 8.978 [0.000, 47.000], loss: 0.314791, mae: 8.719262, mean_q: 13.079865\n",
      " 20989/50000: episode: 95, duration: 3.546s, episode steps: 483, steps per second: 136, episode reward: 25.000, mean reward: 0.052 [0.000, 5.000], mean action: 0.973 [0.000, 2.000], mean observation: 8.453 [0.000, 47.000], loss: 0.309022, mae: 8.723927, mean_q: 13.080490\n",
      " 21983/50000: episode: 96, duration: 7.520s, episode steps: 994, steps per second: 132, episode reward: 40.000, mean reward: 0.040 [0.000, 5.000], mean action: 0.989 [0.000, 2.000], mean observation: 7.916 [0.000, 47.000], loss: 0.270039, mae: 8.780090, mean_q: 13.182494\n",
      " 22452/50000: episode: 97, duration: 4.469s, episode steps: 469, steps per second: 105, episode reward: 25.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.966 [0.000, 2.000], mean observation: 8.297 [0.000, 47.000], loss: 0.282770, mae: 8.832362, mean_q: 13.262421\n",
      " 22565/50000: episode: 98, duration: 0.797s, episode steps: 113, steps per second: 142, episode reward: 5.000, mean reward: 0.044 [0.000, 5.000], mean action: 1.044 [0.000, 2.000], mean observation: 8.920 [0.000, 47.000], loss: 0.248731, mae: 8.803546, mean_q: 13.213483\n",
      " 22946/50000: episode: 99, duration: 2.721s, episode steps: 381, steps per second: 140, episode reward: 20.000, mean reward: 0.052 [0.000, 5.000], mean action: 0.979 [0.000, 2.000], mean observation: 8.491 [0.000, 47.000], loss: 0.285406, mae: 8.856719, mean_q: 13.291831\n",
      " 23052/50000: episode: 100, duration: 0.781s, episode steps: 106, steps per second: 136, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.019 [0.000, 2.000], mean observation: 9.985 [0.000, 47.000], loss: 0.205358, mae: 8.831188, mean_q: 13.267688\n",
      " 23915/50000: episode: 101, duration: 7.110s, episode steps: 863, steps per second: 121, episode reward: 40.000, mean reward: 0.046 [0.000, 5.000], mean action: 1.052 [0.000, 2.000], mean observation: 8.254 [0.000, 47.000], loss: 0.256094, mae: 8.946795, mean_q: 13.434722\n",
      " 24027/50000: episode: 102, duration: 1.084s, episode steps: 112, steps per second: 103, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.955 [0.000, 2.000], mean observation: 9.839 [0.000, 47.000], loss: 0.288208, mae: 9.024595, mean_q: 13.543904\n 24045/50000: episode: 103, duration: 0.189s, episode steps: 18, steps per second: 95, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.111 [0.000, 2.000], mean observation: 12.233 [0.000, 47.000], loss: 0.259702, mae: 9.121036, mean_q: 13.664532\n",
      " 24530/50000: episode: 104, duration: 3.570s, episode steps: 485, steps per second: 136, episode reward: 25.000, mean reward: 0.052 [0.000, 5.000], mean action: 0.969 [0.000, 2.000], mean observation: 8.576 [0.000, 47.000], loss: 0.302715, mae: 9.005281, mean_q: 13.513974\n",
      " 24825/50000: episode: 105, duration: 2.491s, episode steps: 295, steps per second: 118, episode reward: 15.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.966 [0.000, 2.000], mean observation: 8.522 [0.000, 47.000], loss: 0.261383, mae: 9.047605, mean_q: 13.579737\n",
      " 25758/50000: episode: 106, duration: 7.056s, episode steps: 933, steps per second: 132, episode reward: 45.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.972 [0.000, 2.000], mean observation: 8.050 [0.000, 46.000], loss: 0.289488, mae: 9.057335, mean_q: 13.588399\n",
      " 25856/50000: episode: 107, duration: 0.720s, episode steps: 98, steps per second: 136, episode reward: 5.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.867 [0.000, 2.000], mean observation: 9.214 [0.000, 47.000], loss: 0.271934, mae: 9.101006, mean_q: 13.664612\n",
      " 26605/50000: episode: 108, duration: 5.084s, episode steps: 749, steps per second: 147, episode reward: 45.000, mean reward: 0.060 [0.000, 5.000], mean action: 1.035 [0.000, 2.000], mean observation: 8.392 [0.000, 45.000], loss: 0.306986, mae: 9.111136, mean_q: 13.679862\n",
      " 26910/50000: episode: 109, duration: 2.297s, episode steps: 305, steps per second: 133, episode reward: 15.000, mean reward: 0.049 [0.000, 5.000], mean action: 0.984 [0.000, 2.000], mean observation: 8.896 [0.000, 47.000], loss: 0.263474, mae: 9.169834, mean_q: 13.769669\n",
      " 27019/50000: episode: 110, duration: 0.921s, episode steps: 109, steps per second: 118, episode reward: 5.000, mean reward: 0.046 [0.000, 5.000], mean action: 1.046 [0.000, 2.000], mean observation: 9.506 [0.000, 47.000], loss: 0.323436, mae: 9.173691, mean_q: 13.762004\n",
      " 27794/50000: episode: 111, duration: 6.137s, episode steps: 775, steps per second: 126, episode reward: 40.000, mean reward: 0.052 [0.000, 5.000], mean action: 0.942 [0.000, 2.000], mean observation: 7.790 [0.000, 47.000], loss: 0.286329, mae: 9.215143, mean_q: 13.835352\n",
      " 28389/50000: episode: 112, duration: 3.980s, episode steps: 595, steps per second: 149, episode reward: 45.000, mean reward: 0.076 [0.000, 5.000], mean action: 0.998 [0.000, 2.000], mean observation: 7.988 [0.000, 46.000], loss: 0.274289, mae: 9.264673, mean_q: 13.913445\n",
      " 29992/50000: episode: 113, duration: 11.676s, episode steps: 1603, steps per second: 137, episode reward: 45.000, mean reward: 0.028 [0.000, 5.000], mean action: 0.988 [0.000, 2.000], mean observation: 7.277 [0.000, 45.000], loss: 0.286349, mae: 9.306130, mean_q: 13.959963\n 30010/50000: episode: 114, duration: 0.133s, episode steps: 18, steps per second: 136, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.389 [0.000, 2.000], mean observation: 12.256 [0.000, 47.000], loss: 0.299827, mae: 9.327358, mean_q: 13.925164\n",
      " 30442/50000: episode: 115, duration: 3.683s, episode steps: 432, steps per second: 117, episode reward: 30.000, mean reward: 0.069 [0.000, 5.000], mean action: 1.014 [0.000, 2.000], mean observation: 8.092 [0.000, 47.000], loss: 0.240086, mae: 9.262587, mean_q: 13.904504\n",
      " 30724/50000: episode: 116, duration: 1.935s, episode steps: 282, steps per second: 146, episode reward: 15.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.996 [0.000, 2.000], mean observation: 8.630 [0.000, 47.000], loss: 0.238876, mae: 9.275074, mean_q: 13.928208\n",
      " 31424/50000: episode: 117, duration: 4.795s, episode steps: 700, steps per second: 146, episode reward: 40.000, mean reward: 0.057 [0.000, 5.000], mean action: 1.014 [0.000, 2.000], mean observation: 8.133 [0.000, 47.000], loss: 0.265234, mae: 9.253959, mean_q: 13.880090\n",
      " 31810/50000: episode: 118, duration: 2.705s, episode steps: 386, steps per second: 143, episode reward: 20.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.078 [0.000, 2.000], mean observation: 8.841 [0.000, 47.000], loss: 0.276389, mae: 9.151986, mean_q: 13.713941\n 31824/50000: episode: 119, duration: 0.105s, episode steps: 14, steps per second: 134, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.714 [0.000, 2.000], mean observation: 12.257 [0.000, 47.000], loss: 0.212922, mae: 9.121888, mean_q: 13.670951\n",
      " 32424/50000: episode: 120, duration: 4.767s, episode steps: 600, steps per second: 126, episode reward: 25.000, mean reward: 0.042 [0.000, 5.000], mean action: 0.998 [0.000, 2.000], mean observation: 8.526 [0.000, 47.000], loss: 0.261490, mae: 9.128798, mean_q: 13.699201\n",
      " 32735/50000: episode: 121, duration: 2.520s, episode steps: 311, steps per second: 123, episode reward: 15.000, mean reward: 0.048 [0.000, 5.000], mean action: 1.080 [0.000, 2.000], mean observation: 8.877 [0.000, 47.000], loss: 0.227421, mae: 9.157900, mean_q: 13.735616\n",
      " 33040/50000: episode: 122, duration: 2.699s, episode steps: 305, steps per second: 113, episode reward: 15.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.039 [0.000, 2.000], mean observation: 8.595 [0.000, 47.000], loss: 0.249222, mae: 9.120834, mean_q: 13.677597\n",
      " 33702/50000: episode: 123, duration: 4.703s, episode steps: 662, steps per second: 141, episode reward: 45.000, mean reward: 0.068 [0.000, 5.000], mean action: 0.949 [0.000, 2.000], mean observation: 7.423 [0.000, 46.000], loss: 0.267982, mae: 9.073422, mean_q: 13.602404\n",
      " 34345/50000: episode: 124, duration: 4.517s, episode steps: 643, steps per second: 142, episode reward: 40.000, mean reward: 0.062 [0.000, 5.000], mean action: 0.942 [0.000, 2.000], mean observation: 8.385 [0.000, 47.000], loss: 0.223965, mae: 9.017308, mean_q: 13.527681\n",
      " 34621/50000: episode: 125, duration: 1.919s, episode steps: 276, steps per second: 144, episode reward: 15.000, mean reward: 0.054 [0.000, 5.000], mean action: 0.924 [0.000, 2.000], mean observation: 8.216 [0.000, 47.000], loss: 0.298278, mae: 8.996799, mean_q: 13.494678\n",
      " 34810/50000: episode: 126, duration: 1.330s, episode steps: 189, steps per second: 142, episode reward: 10.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.984 [0.000, 2.000], mean observation: 8.411 [0.000, 47.000], loss: 0.195191, mae: 8.918111, mean_q: 13.395293\n",
      " 34917/50000: episode: 127, duration: 0.773s, episode steps: 107, steps per second: 138, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.065 [0.000, 2.000], mean observation: 9.830 [0.000, 47.000], loss: 0.282714, mae: 8.947296, mean_q: 13.408157\n",
      " 35024/50000: episode: 128, duration: 0.783s, episode steps: 107, steps per second: 137, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.037 [0.000, 2.000], mean observation: 9.049 [0.000, 47.000], loss: 0.182440, mae: 8.958347, mean_q: 13.422957\n",
      " 35307/50000: episode: 129, duration: 1.981s, episode steps: 283, steps per second: 143, episode reward: 15.000, mean reward: 0.053 [0.000, 5.000], mean action: 1.025 [0.000, 2.000], mean observation: 8.466 [0.000, 47.000], loss: 0.221215, mae: 8.867268, mean_q: 13.295342\n",
      " 35628/50000: episode: 130, duration: 2.573s, episode steps: 321, steps per second: 125, episode reward: 15.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.081 [0.000, 2.000], mean observation: 8.633 [0.000, 47.000], loss: 0.260852, mae: 8.830566, mean_q: 13.227665\n",
      " 35829/50000: episode: 131, duration: 1.795s, episode steps: 201, steps per second: 112, episode reward: 10.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.030 [0.000, 2.000], mean observation: 8.604 [0.000, 47.000], loss: 0.230165, mae: 8.795067, mean_q: 13.184379\n",
      " 36115/50000: episode: 132, duration: 2.944s, episode steps: 286, steps per second: 97, episode reward: 15.000, mean reward: 0.052 [0.000, 5.000], mean action: 0.969 [0.000, 2.000], mean observation: 8.508 [0.000, 47.000], loss: 0.192961, mae: 8.792434, mean_q: 13.182636\n",
      " 36228/50000: episode: 133, duration: 1.109s, episode steps: 113, steps per second: 102, episode reward: 5.000, mean reward: 0.044 [0.000, 5.000], mean action: 1.124 [0.000, 2.000], mean observation: 9.260 [0.000, 47.000], loss: 0.171237, mae: 8.762004, mean_q: 13.161078\n",
      " 36919/50000: episode: 134, duration: 6.463s, episode steps: 691, steps per second: 107, episode reward: 35.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.006 [0.000, 2.000], mean observation: 8.272 [0.000, 47.000], loss: 0.199586, mae: 8.753794, mean_q: 13.132589\n",
      " 37209/50000: episode: 135, duration: 2.963s, episode steps: 290, steps per second: 98, episode reward: 15.000, mean reward: 0.052 [0.000, 5.000], mean action: 0.962 [0.000, 2.000], mean observation: 8.395 [0.000, 47.000], loss: 0.243341, mae: 8.712140, mean_q: 13.051086\n",
      " 37402/50000: episode: 136, duration: 1.648s, episode steps: 193, steps per second: 117, episode reward: 10.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.109 [0.000, 2.000], mean observation: 8.366 [0.000, 47.000], loss: 0.221263, mae: 8.668046, mean_q: 13.003772\n",
      " 37874/50000: episode: 137, duration: 4.332s, episode steps: 472, steps per second: 109, episode reward: 25.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.977 [0.000, 2.000], mean observation: 8.197 [0.000, 47.000], loss: 0.231164, mae: 8.633406, mean_q: 12.955186\n",
      " 38248/50000: episode: 138, duration: 3.141s, episode steps: 374, steps per second: 119, episode reward: 20.000, mean reward: 0.053 [0.000, 5.000], mean action: 1.016 [0.000, 2.000], mean observation: 8.343 [0.000, 47.000], loss: 0.235998, mae: 8.608686, mean_q: 12.901307\n",
      " 38590/50000: episode: 139, duration: 2.747s, episode steps: 342, steps per second: 125, episode reward: 45.000, mean reward: 0.132 [0.000, 5.000], mean action: 0.977 [0.000, 2.000], mean observation: 6.880 [0.000, 46.000], loss: 0.206922, mae: 8.588417, mean_q: 12.878311\n",
      " 38701/50000: episode: 140, duration: 1.096s, episode steps: 111, steps per second: 101, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.955 [0.000, 2.000], mean observation: 9.951 [0.000, 47.000], loss: 0.194687, mae: 8.598691, mean_q: 12.893360\n",
      " 39673/50000: episode: 141, duration: 7.654s, episode steps: 972, steps per second: 127, episode reward: 45.000, mean reward: 0.046 [0.000, 5.000], mean action: 0.978 [0.000, 2.000], mean observation: 7.444 [0.000, 46.000], loss: 0.237538, mae: 8.613034, mean_q: 12.930303\n 39687/50000: episode: 142, duration: 0.098s, episode steps: 14, steps per second: 142, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.071 [0.000, 2.000], mean observation: 11.500 [1.000, 47.000], loss: 0.070919, mae: 8.604481, mean_q: 12.952749\n",
      " 39907/50000: episode: 143, duration: 1.691s, episode steps: 220, steps per second: 130, episode reward: 10.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.977 [0.000, 2.000], mean observation: 8.777 [0.000, 47.000], loss: 0.248678, mae: 8.648459, mean_q: 12.985178\n 39920/50000: episode: 144, duration: 0.102s, episode steps: 13, steps per second: 128, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.000 [0.000, 2.000], mean observation: 11.185 [0.000, 47.000], loss: 0.277320, mae: 8.739362, mean_q: 13.106781\n",
      " 40140/50000: episode: 145, duration: 1.673s, episode steps: 220, steps per second: 132, episode reward: 10.000, mean reward: 0.045 [0.000, 5.000], mean action: 1.027 [0.000, 2.000], mean observation: 8.591 [0.000, 47.000], loss: 0.250807, mae: 8.638155, mean_q: 12.968324\n",
      " 40369/50000: episode: 146, duration: 1.679s, episode steps: 229, steps per second: 136, episode reward: 20.000, mean reward: 0.087 [0.000, 5.000], mean action: 1.061 [0.000, 2.000], mean observation: 8.504 [0.000, 47.000], loss: 0.209670, mae: 8.675839, mean_q: 13.037003\n",
      " 40562/50000: episode: 147, duration: 1.355s, episode steps: 193, steps per second: 142, episode reward: 10.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.052 [0.000, 2.000], mean observation: 8.775 [0.000, 47.000], loss: 0.256128, mae: 8.650187, mean_q: 12.980983\n",
      " 41310/50000: episode: 148, duration: 5.422s, episode steps: 748, steps per second: 138, episode reward: 45.000, mean reward: 0.060 [0.000, 5.000], mean action: 1.075 [0.000, 2.000], mean observation: 7.660 [0.000, 47.000], loss: 0.216849, mae: 8.652060, mean_q: 12.990351\n",
      " 41419/50000: episode: 149, duration: 0.842s, episode steps: 109, steps per second: 129, episode reward: 5.000, mean reward: 0.046 [0.000, 5.000], mean action: 1.156 [0.000, 2.000], mean observation: 9.554 [0.000, 47.000], loss: 0.181508, mae: 8.611926, mean_q: 12.924588\n",
      " 41623/50000: episode: 150, duration: 1.430s, episode steps: 204, steps per second: 143, episode reward: 10.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.020 [0.000, 2.000], mean observation: 9.009 [0.000, 47.000], loss: 0.242415, mae: 8.641257, mean_q: 12.952363\n",
      " 41823/50000: episode: 151, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: 10.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.070 [0.000, 2.000], mean observation: 9.458 [0.000, 47.000], loss: 0.213654, mae: 8.610870, mean_q: 12.915043\n",
      " 41932/50000: episode: 152, duration: 0.799s, episode steps: 109, steps per second: 136, episode reward: 5.000, mean reward: 0.046 [0.000, 5.000], mean action: 1.028 [0.000, 2.000], mean observation: 10.198 [0.000, 47.000], loss: 0.234594, mae: 8.593608, mean_q: 12.878780\n",
      " 42246/50000: episode: 153, duration: 2.150s, episode steps: 314, steps per second: 146, episode reward: 15.000, mean reward: 0.048 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 8.403 [0.000, 47.000], loss: 0.222358, mae: 8.549108, mean_q: 12.823298\n",
      " 42446/50000: episode: 154, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: 10.000, mean reward: 0.050 [0.000, 5.000], mean action: 0.995 [0.000, 2.000], mean observation: 8.924 [0.000, 47.000], loss: 0.234966, mae: 8.526846, mean_q: 12.782107\n",
      " 42556/50000: episode: 155, duration: 1.023s, episode steps: 110, steps per second: 108, episode reward: 5.000, mean reward: 0.045 [0.000, 5.000], mean action: 0.964 [0.000, 2.000], mean observation: 9.484 [0.000, 47.000], loss: 0.155363, mae: 8.535734, mean_q: 12.807616\n 42571/50000: episode: 156, duration: 0.115s, episode steps: 15, steps per second: 130, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.333 [0.000, 2.000], mean observation: 11.480 [1.000, 47.000], loss: 0.097372, mae: 8.492021, mean_q: 12.748886\n",
      " 42772/50000: episode: 157, duration: 1.662s, episode steps: 201, steps per second: 121, episode reward: 10.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 9.350 [0.000, 47.000], loss: 0.263750, mae: 8.483446, mean_q: 12.707954\n",
      " 42986/50000: episode: 158, duration: 1.793s, episode steps: 214, steps per second: 119, episode reward: 10.000, mean reward: 0.047 [0.000, 5.000], mean action: 0.949 [0.000, 2.000], mean observation: 8.667 [0.000, 47.000], loss: 0.213158, mae: 8.444085, mean_q: 12.651787\n",
      " 43090/50000: episode: 159, duration: 0.886s, episode steps: 104, steps per second: 117, episode reward: 5.000, mean reward: 0.048 [0.000, 5.000], mean action: 1.029 [0.000, 2.000], mean observation: 9.288 [0.000, 47.000], loss: 0.229083, mae: 8.409675, mean_q: 12.592203\n",
      " 43301/50000: episode: 160, duration: 1.870s, episode steps: 211, steps per second: 113, episode reward: 10.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.038 [0.000, 2.000], mean observation: 8.900 [0.000, 47.000], loss: 0.203656, mae: 8.391189, mean_q: 12.578109\n",
      " 43496/50000: episode: 161, duration: 1.683s, episode steps: 195, steps per second: 116, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.990 [0.000, 2.000], mean observation: 8.903 [0.000, 47.000], loss: 0.200107, mae: 8.352735, mean_q: 12.538113\n",
      " 43703/50000: episode: 162, duration: 1.811s, episode steps: 207, steps per second: 114, episode reward: 10.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.986 [0.000, 2.000], mean observation: 8.831 [0.000, 47.000], loss: 0.202707, mae: 8.344140, mean_q: 12.517796\n",
      " 44318/50000: episode: 163, duration: 5.308s, episode steps: 615, steps per second: 116, episode reward: 25.000, mean reward: 0.041 [0.000, 5.000], mean action: 1.094 [0.000, 2.000], mean observation: 8.804 [0.000, 47.000], loss: 0.212548, mae: 8.257036, mean_q: 12.375735\n",
      " 44424/50000: episode: 164, duration: 1.141s, episode steps: 106, steps per second: 93, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 0.925 [0.000, 2.000], mean observation: 9.823 [0.000, 47.000], loss: 0.195749, mae: 8.125856, mean_q: 12.173996\n 44438/50000: episode: 165, duration: 0.109s, episode steps: 14, steps per second: 129, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.786 [0.000, 2.000], mean observation: 12.214 [0.000, 47.000], loss: 0.133997, mae: 8.081201, mean_q: 12.128949\n",
      " 44630/50000: episode: 166, duration: 1.763s, episode steps: 192, steps per second: 109, episode reward: 10.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.141 [0.000, 2.000], mean observation: 9.279 [0.000, 47.000], loss: 0.183293, mae: 8.124638, mean_q: 12.196289\n",
      " 44915/50000: episode: 167, duration: 2.534s, episode steps: 285, steps per second: 112, episode reward: 15.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.951 [0.000, 2.000], mean observation: 8.976 [0.000, 47.000], loss: 0.169800, mae: 8.110975, mean_q: 12.170296\n",
      " 45202/50000: episode: 168, duration: 2.136s, episode steps: 287, steps per second: 134, episode reward: 15.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.031 [0.000, 2.000], mean observation: 8.930 [0.000, 47.000], loss: 0.206312, mae: 8.091519, mean_q: 12.132249\n",
      " 45719/50000: episode: 169, duration: 4.322s, episode steps: 517, steps per second: 120, episode reward: 25.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.981 [0.000, 2.000], mean observation: 8.518 [0.000, 47.000], loss: 0.198028, mae: 8.079180, mean_q: 12.111442\n",
      " 46552/50000: episode: 170, duration: 6.564s, episode steps: 833, steps per second: 127, episode reward: 45.000, mean reward: 0.054 [0.000, 5.000], mean action: 1.013 [0.000, 2.000], mean observation: 8.512 [0.000, 46.000], loss: 0.169333, mae: 8.000875, mean_q: 12.006385\n",
      " 46768/50000: episode: 171, duration: 1.920s, episode steps: 216, steps per second: 113, episode reward: 10.000, mean reward: 0.046 [0.000, 5.000], mean action: 1.009 [0.000, 2.000], mean observation: 9.105 [0.000, 47.000], loss: 0.176919, mae: 7.941337, mean_q: 11.900978\n",
      " 46968/50000: episode: 172, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: 10.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.005 [0.000, 2.000], mean observation: 8.709 [0.000, 47.000], loss: 0.149397, mae: 7.879301, mean_q: 11.821489\n",
      " 47631/50000: episode: 173, duration: 4.733s, episode steps: 663, steps per second: 140, episode reward: 45.000, mean reward: 0.068 [0.000, 5.000], mean action: 0.997 [0.000, 2.000], mean observation: 8.331 [0.000, 45.000], loss: 0.167470, mae: 7.853363, mean_q: 11.775826\n",
      " 47826/50000: episode: 174, duration: 1.320s, episode steps: 195, steps per second: 148, episode reward: 10.000, mean reward: 0.051 [0.000, 5.000], mean action: 0.974 [0.000, 2.000], mean observation: 9.046 [0.000, 47.000], loss: 0.160531, mae: 7.788537, mean_q: 11.676002\n",
      " 48013/50000: episode: 175, duration: 1.268s, episode steps: 187, steps per second: 148, episode reward: 10.000, mean reward: 0.053 [0.000, 5.000], mean action: 0.904 [0.000, 2.000], mean observation: 8.082 [0.000, 47.000], loss: 0.197381, mae: 7.781595, mean_q: 11.666650\n",
      " 48120/50000: episode: 176, duration: 0.942s, episode steps: 107, steps per second: 114, episode reward: 5.000, mean reward: 0.047 [0.000, 5.000], mean action: 0.963 [0.000, 2.000], mean observation: 8.492 [0.000, 47.000], loss: 0.157159, mae: 7.727083, mean_q: 11.596865\n",
      " 48584/50000: episode: 177, duration: 4.830s, episode steps: 464, steps per second: 96, episode reward: 25.000, mean reward: 0.054 [0.000, 5.000], mean action: 0.948 [0.000, 2.000], mean observation: 8.720 [0.000, 47.000], loss: 0.186335, mae: 7.688732, mean_q: 11.531054\n",
      " 48970/50000: episode: 178, duration: 4.734s, episode steps: 386, steps per second: 82, episode reward: 20.000, mean reward: 0.052 [0.000, 5.000], mean action: 1.026 [0.000, 2.000], mean observation: 8.459 [0.000, 47.000], loss: 0.169955, mae: 7.636794, mean_q: 11.455723\n",
      " 49451/50000: episode: 179, duration: 4.921s, episode steps: 481, steps per second: 98, episode reward: 25.000, mean reward: 0.052 [0.000, 5.000], mean action: 0.929 [0.000, 2.000], mean observation: 8.713 [0.000, 47.000], loss: 0.160872, mae: 7.600115, mean_q: 11.398862\n",
      "done, took 381.947 seconds\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Configure and compile the RL agent\n",
    "nb_steps = 60000\n",
    "memory = SequentialMemory(limit=50000, window_length=window_length)\n",
    "# policy = BoltzmannQPolicy()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.0, nb_steps=nb_steps * 0.4)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# learn\n",
    "dqn.fit(env, nb_steps=nb_steps, visualize=False, verbose=2)\n",
    "\n",
    "# save \n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(\"breakout-n\"), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Testing for 5 episodes ...\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "Exception ignored in: <function Monitor.__del__ at 0x7fc077ecb048>\nTraceback (most recent call last):\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 229, in __del__\n    self.close()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 140, in close\n    self._close_video_recorder()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 212, in _close_video_recorder\n    self.video_recorder.close()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\", line 129, in close\n    os.remove(self.path)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/marcofavorito/workfolder/gym-breakout-pygame/examples/openaigym.video.2.7936.video000000.mp4'\nException ignored in: <function Monitor.__del__ at 0x7fc077ecb048>\nTraceback (most recent call last):\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 229, in __del__\n    self.close()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 140, in close\n    self._close_video_recorder()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 212, in _close_video_recorder\n    self.video_recorder.close()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\", line 129, in close\n    os.remove(self.path)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/marcofavorito/workfolder/gym-breakout-pygame/examples/openaigym.video.3.7936.video000000.mp4'\nException ignored in: <function Monitor.__del__ at 0x7fc077ecb048>\nTraceback (most recent call last):\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 229, in __del__\n    self.close()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 140, in close\n    self._close_video_recorder()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\", line 212, in _close_video_recorder\n    self.video_recorder.close()\n  File \"/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\", line 129, in close\n    os.remove(self.path)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/marcofavorito/workfolder/gym-breakout-pygame/examples/openaigym.video.4.7936.video000000.mp4'\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-60b788992a37>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# Evaluate for 5 episodes.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdqn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMonitor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\".\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnb_episodes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvisualize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/rl/core.py\u001B[0m in \u001B[0;36mtest\u001B[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001B[0m\n\u001B[1;32m    307\u001B[0m             \u001B[0;31m# Obtain the initial observation by resetting the environment.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    308\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_states\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 309\u001B[0;31m             \u001B[0mobservation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    310\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocessor\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    311\u001B[0m                 \u001B[0mobservation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocessor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_observation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001B[0m in \u001B[0;36mreset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_before_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0mobservation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_after_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001B[0m in \u001B[0;36m_after_reset\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m    186\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstats_recorder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mafter_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 188\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_video_recorder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    189\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m         \u001B[0;31m# Bump *after* all reset activity has finished\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001B[0m in \u001B[0;36mreset_video_recorder\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    207\u001B[0m             \u001B[0menabled\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_video_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    208\u001B[0m         )\n\u001B[0;32m--> 209\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvideo_recorder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcapture_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    210\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    211\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_close_video_recorder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001B[0m in \u001B[0;36mcapture_frame\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0mrender_mode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'ansi'\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mansi_mode\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m'rgb_array'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m         \u001B[0mframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrender_mode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mframe\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workfolder/gym-breakout-pygame/gym_breakout_pygame/breakout_env.py\u001B[0m in \u001B[0;36mrender\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    556\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mviewer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPygameViewer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    557\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 558\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mviewer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    559\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workfolder/gym-breakout-pygame/gym_breakout_pygame/breakout_env.py\u001B[0m in \u001B[0;36mrender\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"human\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fill_screen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     73\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_draw_score_label\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_draw_last_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workfolder/gym-breakout-pygame/gym_breakout_pygame/breakout_env.py\u001B[0m in \u001B[0;36m_fill_screen\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fill_screen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscreen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfill\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwhite\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_draw_score_label\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31merror\u001B[0m: display Surface quit"
     ],
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Evaluate for 5 episodes.\n",
    "dqn.test(Monitor(env, \".\", force=True), nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Now check the `examples/` folder, you should be able to see the recordings of the learned policy.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
